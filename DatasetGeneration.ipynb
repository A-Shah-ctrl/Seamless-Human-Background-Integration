{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.*"
      ],
      "metadata": {
        "id": "pWO5OkfHG5hY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPbB7s8hPHfO"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "_Note: In Google Colab, you need to restart the runtime to reload the modules you installed in the previous section. Else, you will get an error. You can do so by selecting `Runtime > Restart Runtime` in the Menu bar. **Please do not run the cell below without restarting.**_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2yTMLA0yjpP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pwmP93l9IAu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Compile the Object Detection API protocol buffers and install the necessary packages\n",
        "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EtGbyNc8VgS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "from google.colab .patches import cv2_imshow\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbVcXrMay38S"
      },
      "outputs": [],
      "source": [
        "model_display_name = 'Mask R-CNN Inception ResNet V2 1024x1024'\n",
        "model_handle = 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
        "\n",
        "print('Selected model:'+ model_display_name)\n",
        "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7nTrTOfzNsF"
      },
      "outputs": [],
      "source": [
        "# This will take 10 to 15 minutes to finish\n",
        "print('loading model...')\n",
        "hub_model = hub.load(model_handle)\n",
        "print('model loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the images we deal with will be resized to 400 x 400 pixels"
      ],
      "metadata": {
        "id": "BCYV9DPtyFF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HEIGHT = 400\n",
        "WIDTH = 400"
      ],
      "metadata": {
        "id": "FarjouhKyEXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating two folders for background and foreground images. The folder **back** will contain all the indoor backgrounds. The folder **front** will contain all the pedestrian images."
      ],
      "metadata": {
        "id": "0kljUYaSyOrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir front\n",
        "!mkdir back"
      ],
      "metadata": {
        "id": "qkUDgVn_xQAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask Extraction\n",
        "\n",
        "We obtain a masks of objects detected in the outdoor image from the Mask R-CNN Inception ResNet V2. From all the objects detected, we only take the masks of class 1 which represent \"humans\". Any mask that is less than 9% of the total image area is discarded. If there are more that two human masks we only keep the first two and discard the rest."
      ],
      "metadata": {
        "id": "IHPAnUnMNqdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_multiple_mask(image):\n",
        "  # run inference\n",
        "  results = hub_model(image)\n",
        "\n",
        "  # output values are tensors and we only need the numpy()\n",
        "  # parameter when we visualize the results\n",
        "  result = {key:value.numpy() for key,value in results.items()}\n",
        "  if 'detection_masks' in result:\n",
        "    # convert np.arrays to tensors\n",
        "    classes = results['detection_classes'][0][:2].numpy()\n",
        "    detection_masks = tf.convert_to_tensor(result['detection_masks'][0])\n",
        "    detection_boxes = tf.convert_to_tensor(result['detection_boxes'][0])\n",
        "\n",
        "    # reframe the the bounding box mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              detection_masks, detection_boxes,\n",
        "                image.shape[1], image.shape[2])\n",
        "\n",
        "    # filter mask pixel values that are above a specified threshold\n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.6,\n",
        "                                        tf.uint8)\n",
        "\n",
        "    # get the numpy array\n",
        "    result['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    masks = []\n",
        "\n",
        "    #cls == 1 represents the \"human\" class\n",
        "    i = 0\n",
        "    for cls in classes:\n",
        "      if cls == 1:\n",
        "        mask = np.array(result['detection_masks_reframed'][i]) * 255\n",
        "        mask = cv2.resize(mask, (WIDTH, HEIGHT))\n",
        "        masks.append(mask)\n",
        "        i += 1\n",
        "\n",
        "    # Removing any masks that are <9% of the image area\n",
        "    proper_masks = []\n",
        "    for mask in masks:\n",
        "      xmin, ymin, w, h = cv2.boundingRect(mask) # getting the bounding area of the mask\n",
        "      xmax = xmin + w\n",
        "      ymax = ymin + h\n",
        "      area = (xmax-xmin) * (ymax-ymin)\n",
        "      if area/(HEIGHT * WIDTH) < 0.09:\n",
        "        continue\n",
        "      else:\n",
        "        proper_masks.append(mask)\n",
        "\n",
        "    if len(proper_masks) == 0:\n",
        "      print(\"No proper mask found .. skipping this image\")\n",
        "      return [np.zeros((WIDTH,HEIGHT))] # A list with blank masks is returned\n",
        "\n",
        "    elif len(proper_masks) == 1:\n",
        "      return [proper_masks[0]]\n",
        "\n",
        "    else:\n",
        "      return [proper_masks[0], proper_masks[1]] # only the first two masks are returned"
      ],
      "metadata": {
        "id": "zpo_2FZpMp6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the masks\n",
        "def merge_mask(masks):\n",
        "    full = np.zeros((WIDTH,HEIGHT))\n",
        "    bounding_boxes = []\n",
        "\n",
        "    for mask in masks:\n",
        "      # Greyscaling the mask if it isn't already\n",
        "      if len(mask.shape) == 3:\n",
        "          mask = cv2.cvtColor(mask, cv2.COLOR_RGB2GRAY)\n",
        "      # Convert the mask into binary format\n",
        "      _, mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
        "      mask = mask.astype(np.uint8)\n",
        "      # Get the bounding box for the mask\n",
        "      x, y, w, h = cv2.boundingRect(mask)\n",
        "      bounding_boxes.append((x, y, w, h))\n",
        "\n",
        "      # Merge the mask with the full mask\n",
        "      full = full + mask\n",
        "\n",
        "    blurred = cv2.GaussianBlur(full, (5, 5), 0)\n",
        "    return blurred, bounding_boxes"
      ],
      "metadata": {
        "id": "XogO0QZm0UMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_input_image(path, height,width):\n",
        "  image = cv2.imread(path)\n",
        "  image = cv2.resize(image, (height, width))\n",
        "\n",
        "  return image, np.array(image.reshape(\n",
        "    (1, height, width, 3)).astype(np.uint8))"
      ],
      "metadata": {
        "id": "RQ2bM9QCIAmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Cut and Paste"
      ],
      "metadata": {
        "id": "ZtJcaCJAGKyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_simple_cutouts(input_image, target_image, final_mask, bounding_box):\n",
        "\n",
        "    # Convert the mask to 3D\n",
        "    mask_3d = cv2.merge([final_mask,final_mask,final_mask])/255\n",
        "    # Blended image\n",
        "    blended = input_image * mask_3d + target_image * (1 - mask_3d)\n",
        "    blended_box = blended.copy()\n",
        "    # Adding the bounding boxes for the individual human masks\n",
        "    for box in bounding_box:\n",
        "      x, y, w, h = box\n",
        "      cv2.rectangle(blended_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    return blended, blended_box"
      ],
      "metadata": {
        "id": "1vlTmc9oGOuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Poisson Blending"
      ],
      "metadata": {
        "id": "mVGpg9JzQXak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_poisson_image(input_image, target_image, final_mask, bounding_box):\n",
        "    mask_3d = cv2.merge([final_mask,final_mask,final_mask])/255\n",
        "    final_mask = final_mask.astype(np.uint8)\n",
        "    # Blended image\n",
        "    blended = input_image * mask_3d + target_image * (1 - mask_3d)\n",
        "    blended = blended.astype(np.uint8)\n",
        "\n",
        "    # Poisson Blend the image (need to get the start of the whole mask (multiple masks))\n",
        "    x_, y_, w_, h_ = cv2.boundingRect(final_mask)\n",
        "    offset = (x_,y_)\n",
        "    center = (offset[0] + w_ // 2, offset[1] + h_ // 2)\n",
        "    blended = np.clip(cv2.seamlessClone(input_image, target_image, final_mask, center, cv2.NORMAL_CLONE),0,255)\n",
        "    blended_box = blended.copy()\n",
        "\n",
        "    # Adding individual boxes for each mask\n",
        "    for box in bounding_box:\n",
        "      x, y, w, h = box\n",
        "      cv2.rectangle(blended_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    return blended, blended_box\n",
        "\n"
      ],
      "metadata": {
        "id": "3kHzC5EBcRhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laplacian Blending"
      ],
      "metadata": {
        "id": "Lion_eQ3uF2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a Gaussian pyramid\n",
        "def generate_gaussian_pyramid(image, levels):\n",
        "    pyramid = [image]\n",
        "    for i in range(levels - 1):\n",
        "        image = cv2.pyrDown(image)\n",
        "        pyramid.append(image)\n",
        "    return pyramid\n",
        "\n",
        "# Function to generate a Laplacian pyramid\n",
        "def generate_laplacian_pyramid(gaussian_pyramid):\n",
        "    laplacian_pyramid = []\n",
        "    for i in range(len(gaussian_pyramid) - 1):\n",
        "        upsampled = cv2.pyrUp(gaussian_pyramid[i + 1])\n",
        "        laplacian = cv2.subtract(gaussian_pyramid[i], upsampled)\n",
        "        laplacian_pyramid.append(laplacian)\n",
        "    laplacian_pyramid.append(gaussian_pyramid[-1])\n",
        "    return laplacian_pyramid\n",
        "\n",
        "# Function to reconstruct the image from the Laplacian pyramid\n",
        "def reconstruct_image_from_pyramid(laplacian_pyramid):\n",
        "    image = laplacian_pyramid[-1]\n",
        "    for i in range(len(laplacian_pyramid) - 2, -1, -1):\n",
        "        image = cv2.pyrUp(image)\n",
        "        image = cv2.add(image, laplacian_pyramid[i])\n",
        "    return image\n",
        "\n",
        "# Function to perform Laplacian blending of foreground (source) and background (target) images\n",
        "def get_laplacian_image(input_image, target_image, final_mask, bounding_box, levels=3):\n",
        "\n",
        "    # Generate Gaussian pyramids for the source, target, and mask\n",
        "    source_pyramid = generate_gaussian_pyramid(input_image, levels)\n",
        "    target_pyramid = generate_gaussian_pyramid(target_image, levels)\n",
        "    mask_pyramid = generate_gaussian_pyramid(final_mask, levels)\n",
        "\n",
        "    # Generate Laplacian pyramids for the source and target\n",
        "    source_laplacian = generate_laplacian_pyramid(source_pyramid)\n",
        "    target_laplacian = generate_laplacian_pyramid(target_pyramid)\n",
        "\n",
        "\n",
        "    # Blend the Laplacian pyramids at each level using the mask\n",
        "    blended_laplacian = []\n",
        "    for source_lap, target_lap, mask in zip(source_laplacian, target_laplacian, mask_pyramid):\n",
        "        mask_3d = cv2.merge([mask,mask,mask])/255\n",
        "        blended = source_lap * (mask_3d) + target_lap * (1 - (mask_3d))  # Normalize mask to 0-1\n",
        "        blended_laplacian.append(blended)\n",
        "\n",
        "    # Reconstruct the final image from the blended Laplacian pyramids\n",
        "    blended_image = np.clip(reconstruct_image_from_pyramid(blended_laplacian),0,255)\n",
        "    blended_box = blended_image.copy()\n",
        "\n",
        "    for box in bounding_box:\n",
        "      x, y, w, h = box\n",
        "      cv2.rectangle(blended_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    return blended_image, blended_box\n"
      ],
      "metadata": {
        "id": "KJ2NpVOaNS_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Telea Inpaint Blending"
      ],
      "metadata": {
        "id": "IBp9C9zOhwZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inpaint_cutout(input_image, target_image, masks, pr=0.05):\n",
        "\n",
        "    full = np.zeros((WIDTH,HEIGHT))\n",
        "    full_trimap = np.zeros((WIDTH,HEIGHT))\n",
        "    bounding_boxes = []\n",
        "\n",
        "    for mask in masks:\n",
        "      # Greyscaling the mask if it isn't already\n",
        "      if len(mask.shape) == 3:\n",
        "          mask = cv2.cvtColor(mask, cv2.COLOR_RGB2GRAY)\n",
        "      # Convert the mask into binary format\n",
        "      _, mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
        "      mask = mask.astype(np.uint8)\n",
        "      x, y, w, h = cv2.boundingRect(mask)\n",
        "      bounding_boxes.append((x, y, w, h))\n",
        "\n",
        "      # Generate a Trimap from the Mask\n",
        "      trimap = np.zeros_like(mask, dtype=np.uint8)\n",
        "      trimap[mask == 255] = 255 # foreground\n",
        "      trimap[mask == 0] = 0 # background\n",
        "      back = cv2.dilate(mask, np.ones((int(pr*w), int(pr*w)), np.uint8), iterations=1)\n",
        "      fore = cv2.erode(mask, np.ones((int(pr*w),int(pr*w)), np.uint8), iterations=1)\n",
        "\n",
        "      # Adjusting the trimap\n",
        "      bord1 = back - fore\n",
        "      trimap[bord1 == 255] = 128\n",
        "\n",
        "      # For visualization purposes\n",
        "      visual_trimap = cv2.bitwise_or(trimap, fore)\n",
        "      # cv2_imshow(visual_trimap) # Uncomment to visualize what the trimap looks like\n",
        "\n",
        "      # Back to 0 and 255 for computational purposes\n",
        "      trimap[trimap == 255] = 0\n",
        "      trimap[trimap == 128] = 255\n",
        "\n",
        "      full = full + mask\n",
        "      full_trimap = full_trimap + trimap\n",
        "\n",
        "    blurred = cv2.GaussianBlur(full_trimap, (5, 5), 0)\n",
        "    blurred = blurred.astype(np.uint8)\n",
        "    mask_3d = cv2.merge([full,full,full])/255\n",
        "\n",
        "    cutpaste = mask_3d * input_image + (1 - mask_3d) * target_image\n",
        "    cutpaste = cutpaste.astype(np.uint8)\n",
        "\n",
        "    blended_image = cv2.inpaint(cutpaste, blurred, 5, flags=cv2.INPAINT_TELEA)\n",
        "    blended_box = blended_image.copy()\n",
        "\n",
        "    # Adding individual boxes for each mask\n",
        "    for box in bounding_boxes:\n",
        "      x, y, w, h = box\n",
        "      cv2.rectangle(blended_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    return blended_image, blended_box\n"
      ],
      "metadata": {
        "id": "v4mDo6SHh5k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Generation (Multiple Techniques)"
      ],
      "metadata": {
        "id": "XZnHVT5ObjTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir poisson\n",
        "!mkdir laplacian\n",
        "!mkdir cutpaste\n",
        "!mkdir inpaint"
      ],
      "metadata": {
        "id": "BeVcJyxAhe_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# blending - \"all\", \"cutpaste\", \"poisson\", \"laplacian\"\n",
        "def generate_dataset(input_folder=\"front/\", output_folder=\"back/\", blending=\"all\"):\n",
        "\n",
        "    if blending not in [\"all\", \"inpaint\", \"poisson\", \"laplacian\",\"cutpaste\"]:\n",
        "        print(\"Sorry we don't have this blending option.\")\n",
        "        exit()\n",
        "\n",
        "    # Getting all the images\n",
        "    input_images = os.listdir(input_folder)\n",
        "    target_images = os.listdir(output_folder)\n",
        "\n",
        "    for i in range(len(input_images)):\n",
        "      for j in range(len(target_images)):\n",
        "        if \".png\" in input_images[i] and \".jpg\" in target_images[j]:\n",
        "          print(f\"Processing {input_images[i]} and {target_images[j]}\")\n",
        "          input_image, input = load_input_image(\"front/\" + input_images[i],HEIGHT, WIDTH)\n",
        "          target_image = cv2.resize(cv2.imread(\"back/\" + target_images[j]),(HEIGHT,WIDTH))\n",
        "          input_image = input_image.astype(np.uint8)\n",
        "          target_image = target_image.astype(np.uint8)\n",
        "\n",
        "          # Get the mask from the Inception Model\n",
        "          masks = extract_multiple_mask(input)\n",
        "          final_mask, bounding_box = merge_mask(masks)\n",
        "\n",
        "          if blending == \"cutpaste\" or blending == \"all\":\n",
        "              # Simple cut and paste\n",
        "              simple, simple_boxed = get_simple_cutouts(input_image, target_image, final_mask, bounding_box)\n",
        "              # Simple cut and paste dataset\n",
        "              cv2.imwrite(f\"cutpaste/{input_images[i].split('.')[0]}_{target_images[j].split('.')[0]}.jpg\",simple)\n",
        "              cv2.imwrite(f\"cutpaste/{input_images[i].split('.')[0]}_{target_images[j].split('.')[0]}_box.jpg\",simple_boxed)\n",
        "\n",
        "          if blending == \"poisson\" or blending == \"all\":\n",
        "              # Poisson Blending\n",
        "              poisson, poisson_boxed = get_poisson_image(input_image, target_image, final_mask, bounding_box)\n",
        "              # Poisson dataset\n",
        "              cv2.imwrite(f\"poisson/{input_images[i].split('.')[0]}_{target_images[j].split('.')[0]}.jpg\",poisson)\n",
        "              cv2.imwrite(f\"poisson/{input_images[i].split('.')[0]}_{target_images[j].split('.')[0]}_box.jpg\",poisson_boxed)\n",
        "\n",
        "          if blending == \"laplacian\" or blending == \"all\":\n",
        "              # Laplacian Blending\n",
        "              laplacian, laplacian_boxed = get_laplacian_image(input_image, target_image, final_mask, bounding_box)\n",
        "              # Laplacian dataset\n",
        "              cv2.imwrite(f\"laplacian/{input_images[i].split('.')[0]}_{target_images[j].split('.')[0]}.jpg\",laplacian)\n",
        "              cv2.imwrite(f\"laplacian/{input_images[i].split('.')[0]}_{target_images[j].split('.')[0]}_box.jpg\",laplacian_boxed)\n",
        "\n",
        "          if blending == \"inpaint\" or blending == \"all\":\n",
        "              # Telea Inpainting Blending\n",
        "              telea, telea_boxed = get_inpaint_cutout(input_image, target_image, masks)\n",
        "              # Telea dataset\n",
        "              cv2.imwrite(f\"inpaint/{input_images[i].split('.')[0]}_{target_images[j].split('.')[0]}.jpg\",telea)\n",
        "              cv2.imwrite(f\"inpaint/{input_images[i].split('.')[0]}_{target_images[j].split('.')[0]}_box.jpg\",telea_boxed)\n",
        ""
      ],
      "metadata": {
        "id": "QOr-lJrVxGb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating all datasets"
      ],
      "metadata": {
        "id": "CoDV0DN6DuX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_dataset() # will generate all datasets"
      ],
      "metadata": {
        "id": "hNbYTj5aDyy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading zip files for all datasets"
      ],
      "metadata": {
        "id": "V45CHSjKDv6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\"inpaint\".replace('.zip', ''), 'zip', \"inpaint\")\n",
        "zip_filename = \"inpaint.zip\"\n",
        "files.download(zip_filename)\n",
        "\n",
        "shutil.make_archive(\"poisson\".replace('.zip', ''), 'zip', \"poisson\")\n",
        "zip_filename = \"poisson.zip\"\n",
        "files.download(zip_filename)\n",
        "\n",
        "shutil.make_archive(\"laplacian\".replace('.zip', ''), 'zip', \"laplacian\")\n",
        "zip_filename = \"laplacian.zip\"\n",
        "files.download(zip_filename)\n",
        "\n",
        "shutil.make_archive(\"cutpaste\".replace('.zip', ''), 'zip', \"cutpaste\")\n",
        "zip_filename = \"cutpaste.zip\"\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "va-1doC27hQm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}