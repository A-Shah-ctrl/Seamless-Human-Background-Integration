{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pWO5OkfHG5hY"
   },
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPbB7s8hPHfO"
   },
   "source": [
    "## Import libraries\n",
    "\n",
    "_Note: In Google Colab, you need to restart the runtime to reload the modules you installed in the previous section. Else, you will get an error. You can do so by selecting `Runtime > Restart Runtime` in the Menu bar. **Please do not run the cell below without restarting.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Q2yTMLA0yjpP"
   },
   "outputs": [],
   "source": [
    "# Clone the tensorflow models repository\n",
    "!git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8pwmP93l9IAu"
   },
   "outputs": [],
   "source": [
    "# Compile the Object Detection API protocol buffers and install the necessary packages\n",
    "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EtGbyNc8VgS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "from google.colab .patches import cv2_imshow\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbVcXrMay38S"
   },
   "outputs": [],
   "source": [
    "model_display_name = 'Mask R-CNN Inception ResNet V2 1024x1024'\n",
    "model_handle = 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
    "\n",
    "print('Selected model:'+ model_display_name)\n",
    "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7nTrTOfzNsF"
   },
   "outputs": [],
   "source": [
    "# This will take 10 to 15 minutes to finish\n",
    "print('loading model...')\n",
    "hub_model = hub.load(model_handle)\n",
    "print('model loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCYV9DPtyFF8"
   },
   "source": [
    "All the images we deal with will be resized to 400 x 400 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FarjouhKyEXP"
   },
   "outputs": [],
   "source": [
    "HEIGHT = 400\n",
    "WIDTH = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kljUYaSyOrj"
   },
   "source": [
    "Generating two folders for background and foreground images. The folder **back** will contain all the indoor backgrounds. The folder **front** will contain all the pedestrian images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkUDgVn_xQAz"
   },
   "outputs": [],
   "source": [
    "# !mkdir front\n",
    "# !mkdir back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHPAnUnMNqdN"
   },
   "source": [
    "## Mask Extraction\n",
    "\n",
    "We obtain a masks of objects detected in the outdoor image from the Mask R-CNN Inception ResNet V2. From all the objects detected, we only take the masks of class 1 which represent \"humans\". Any mask that is less than 9% of the total image area is discarded. If there are more that two human masks we only keep the first two and discard the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpo_2FZpMp6s"
   },
   "outputs": [],
   "source": [
    "def extract_multiple_mask(image):\n",
    "  # run inference\n",
    "  results = hub_model(image)\n",
    "\n",
    "  # output values are tensors and we only need the numpy()\n",
    "  # parameter when we visualize the results\n",
    "  result = {key:value.numpy() for key,value in results.items()}\n",
    "  if 'detection_masks' in result:\n",
    "    # convert np.arrays to tensors\n",
    "    classes = results['detection_classes'][0][:2].numpy()\n",
    "    detection_masks = tf.convert_to_tensor(result['detection_masks'][0])\n",
    "    detection_boxes = tf.convert_to_tensor(result['detection_boxes'][0])\n",
    "\n",
    "    # reframe the the bounding box mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              detection_masks, detection_boxes,\n",
    "                image.shape[1], image.shape[2])\n",
    "\n",
    "    # filter mask pixel values that are above a specified threshold\n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.6,\n",
    "                                        tf.uint8)\n",
    "\n",
    "    # get the numpy array\n",
    "    result['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    masks = []\n",
    "\n",
    "    #cls == 1 represents the \"human\" class\n",
    "    i = 0\n",
    "    for cls in classes:\n",
    "      if cls == 1:\n",
    "        mask = np.array(result['detection_masks_reframed'][i]) * 255\n",
    "        mask = cv2.resize(mask, (WIDTH, HEIGHT))\n",
    "        masks.append(mask)\n",
    "        i += 1\n",
    "\n",
    "    # Removing any masks that are <9% of the image area\n",
    "    proper_masks = []\n",
    "    for mask in masks:\n",
    "      xmin, ymin, w, h = cv2.boundingRect(mask) # getting the bounding area of the mask\n",
    "      xmax = xmin + w\n",
    "      ymax = ymin + h\n",
    "      area = (xmax-xmin) * (ymax-ymin)\n",
    "      if area/(HEIGHT * WIDTH) < 0.09:\n",
    "        continue\n",
    "      else:\n",
    "        proper_masks.append(mask)\n",
    "\n",
    "    if len(proper_masks) == 0:\n",
    "      print(\"No proper mask found .. skipping this image\")\n",
    "      return [] # A list with blank masks is returned\n",
    "\n",
    "    elif len(proper_masks) == 1:\n",
    "      return [proper_masks[0]]\n",
    "\n",
    "    else:\n",
    "      return [proper_masks[0], proper_masks[1]] # only the first two masks are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XogO0QZm0UMe"
   },
   "outputs": [],
   "source": [
    "# Merging the masks\n",
    "def merge_mask(masks):\n",
    "    full = np.zeros((WIDTH,HEIGHT))\n",
    "    bounding_boxes = []\n",
    "\n",
    "    for mask in masks:\n",
    "      # Greyscaling the mask if it isn't already\n",
    "      if len(mask.shape) == 3:\n",
    "          mask = cv2.cvtColor(mask, cv2.COLOR_RGB2GRAY)\n",
    "      # Convert the mask into binary format\n",
    "      _, mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "      mask = mask.astype(np.uint8)\n",
    "      # Get the bounding box for the mask\n",
    "      x, y, w, h = cv2.boundingRect(mask)\n",
    "      bounding_boxes.append((x, y, w, h))\n",
    "\n",
    "      # Merge the mask with the full mask\n",
    "      full = full + mask\n",
    "\n",
    "    blurred = cv2.GaussianBlur(full, (5, 5), 0)\n",
    "    return blurred, bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQ2bM9QCIAmC"
   },
   "outputs": [],
   "source": [
    "def load_input_image(path, height,width):\n",
    "  image = cv2.imread(path)\n",
    "  image = cv2.resize(image, (height, width))\n",
    "\n",
    "  return image, np.array(image.reshape(\n",
    "    (1, height, width, 3)).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtJcaCJAGKyz"
   },
   "source": [
    "## Simple Cut and Paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vlTmc9oGOuP"
   },
   "outputs": [],
   "source": [
    "def get_simple_cutouts(input_image, target_image, final_mask, bounding_box):\n",
    "\n",
    "    # Convert the mask to 3D\n",
    "    mask_3d = cv2.merge([final_mask,final_mask,final_mask])/255\n",
    "    # Blended image\n",
    "    blended = input_image * mask_3d + target_image * (1 - mask_3d)\n",
    "    blended_box = blended.copy()\n",
    "    # Adding the bounding boxes for the individual human masks\n",
    "    for box in bounding_box:\n",
    "      x, y, w, h = box\n",
    "      cv2.rectangle(blended_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    return blended, blended_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVGpg9JzQXak"
   },
   "source": [
    "## Poisson Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kHzC5EBcRhO"
   },
   "outputs": [],
   "source": [
    "def get_poisson_image(input_image, target_image, final_mask, bounding_box):\n",
    "    mask_3d = cv2.merge([final_mask,final_mask,final_mask])/255\n",
    "    final_mask = final_mask.astype(np.uint8)\n",
    "    # Blended image\n",
    "    blended = input_image * mask_3d + target_image * (1 - mask_3d)\n",
    "    blended = blended.astype(np.uint8)\n",
    "\n",
    "    # Poisson Blend the image (need to get the start of the whole mask (multiple masks))\n",
    "    x_, y_, w_, h_ = cv2.boundingRect(final_mask)\n",
    "    offset = (x_,y_)\n",
    "    center = (offset[0] + w_ // 2, offset[1] + h_ // 2)\n",
    "    blended = np.clip(cv2.seamlessClone(input_image, target_image, final_mask, center, cv2.NORMAL_CLONE),0,255)\n",
    "    blended_box = blended.copy()\n",
    "\n",
    "    # Adding individual boxes for each mask\n",
    "    for box in bounding_box:\n",
    "      x, y, w, h = box\n",
    "      cv2.rectangle(blended_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    return blended, blended_box\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lion_eQ3uF2v"
   },
   "source": [
    "## Laplacian Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJ2NpVOaNS_X"
   },
   "outputs": [],
   "source": [
    "# Function to generate a Gaussian pyramid\n",
    "def generate_gaussian_pyramid(image, levels):\n",
    "    pyramid = [image]\n",
    "    for i in range(levels - 1):\n",
    "        image = cv2.pyrDown(image)\n",
    "        pyramid.append(image)\n",
    "    return pyramid\n",
    "\n",
    "# Function to generate a Laplacian pyramid\n",
    "def generate_laplacian_pyramid(gaussian_pyramid):\n",
    "    laplacian_pyramid = []\n",
    "    for i in range(len(gaussian_pyramid) - 1):\n",
    "        upsampled = cv2.pyrUp(gaussian_pyramid[i + 1])\n",
    "        laplacian = cv2.subtract(gaussian_pyramid[i], upsampled)\n",
    "        laplacian_pyramid.append(laplacian)\n",
    "    laplacian_pyramid.append(gaussian_pyramid[-1])\n",
    "    return laplacian_pyramid\n",
    "\n",
    "# Function to reconstruct the image from the Laplacian pyramid\n",
    "def reconstruct_image_from_pyramid(laplacian_pyramid):\n",
    "    image = laplacian_pyramid[-1]\n",
    "    for i in range(len(laplacian_pyramid) - 2, -1, -1):\n",
    "        image = cv2.pyrUp(image)\n",
    "        image = cv2.add(image, laplacian_pyramid[i])\n",
    "    return image\n",
    "\n",
    "# Function to perform Laplacian blending of foreground (source) and background (target) images\n",
    "def get_laplacian_image(input_image, target_image, final_mask, bounding_box, levels=3):\n",
    "\n",
    "    # Generate Gaussian pyramids for the source, target, and mask\n",
    "    source_pyramid = generate_gaussian_pyramid(input_image, levels)\n",
    "    target_pyramid = generate_gaussian_pyramid(target_image, levels)\n",
    "    mask_pyramid = generate_gaussian_pyramid(final_mask, levels)\n",
    "\n",
    "    # Generate Laplacian pyramids for the source and target\n",
    "    source_laplacian = generate_laplacian_pyramid(source_pyramid)\n",
    "    target_laplacian = generate_laplacian_pyramid(target_pyramid)\n",
    "\n",
    "\n",
    "    # Blend the Laplacian pyramids at each level using the mask\n",
    "    blended_laplacian = []\n",
    "    for source_lap, target_lap, mask in zip(source_laplacian, target_laplacian, mask_pyramid):\n",
    "        mask_3d = cv2.merge([mask,mask,mask])/255\n",
    "        blended = source_lap * (mask_3d) + target_lap * (1 - (mask_3d))  # Normalize mask to 0-1\n",
    "        blended_laplacian.append(blended)\n",
    "\n",
    "    # Reconstruct the final image from the blended Laplacian pyramids\n",
    "    blended_image = np.clip(reconstruct_image_from_pyramid(blended_laplacian),0,255)\n",
    "    blended_box = blended_image.copy()\n",
    "\n",
    "    for box in bounding_box:\n",
    "      x, y, w, h = box\n",
    "      cv2.rectangle(blended_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    return blended_image, blended_box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBp9C9zOhwZD"
   },
   "source": [
    "## Telea Inpaint Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lighting(foreground, background):\n",
    "    # Convert images to LAB color space\n",
    "    fg_lab = cv2.cvtColor(foreground, cv2.COLOR_RGB2LAB)\n",
    "    bg_lab = cv2.cvtColor(background, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "    # # Compute the mean and standard deviation of the L channel (lightness)\n",
    "    l_mean_fg, l_std_fg = cv2.meanStdDev(fg_lab[:,:,0])\n",
    "    l_mean_bg, l_std_bg = cv2.meanStdDev(bg_lab[:,:,0])\n",
    "\n",
    "    # # Adjust the L channel of the foreground\n",
    "    l_channel = fg_lab[:,:,0]\n",
    "    l_channel = ((l_channel - l_mean_fg[0][0]) * (l_std_bg[0][0] / l_std_fg[0][0])) + l_mean_bg[0][0]\n",
    "    l_channel = np.clip(l_channel, 0, 255)\n",
    "    fg_lab[:,:,0] = l_channel\n",
    "    # l_channel = l_channel/4\n",
    "    # fg_lab[:,:,0] = l_channel\n",
    "\n",
    "\n",
    "    # Convert back to RGB color space\n",
    "    adjusted_foreground = cv2.cvtColor(fg_lab, cv2.COLOR_LAB2RGB)\n",
    "    return adjusted_foreground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4mDo6SHh5k4"
   },
   "outputs": [],
   "source": [
    "def get_inpaint_cutout(input_image, target_image, masks, pr=0.0075, light=False):\n",
    "\n",
    "    full = np.zeros((WIDTH,HEIGHT))\n",
    "    full_trimap = np.zeros((WIDTH,HEIGHT))\n",
    "    bounding_boxes = []\n",
    "    if light:\n",
    "      input_image = adjust_lighting(input_image, target_image)\n",
    "    for mask in masks:\n",
    "      # Greyscaling the mask if it isn't already\n",
    "      if len(mask.shape) == 3:\n",
    "          mask = cv2.cvtColor(mask, cv2.COLOR_RGB2GRAY)\n",
    "      # Convert the mask into binary format\n",
    "      _, mask = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "      # Convert images to uint8\n",
    "      input_image = input_image.astype(np.uint8)\n",
    "      target_image = target_image.astype(np.uint8)\n",
    "      mask = mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "      x, y, w, h = cv2.boundingRect(mask)\n",
    "      bounding_boxes.append((x, y, w, h))\n",
    "\n",
    "      # Generate a Trimap from the Mask\n",
    "      trimap = np.zeros_like(mask, dtype=np.uint8)\n",
    "      trimap[mask == 255] = 255 # foreground\n",
    "      trimap[mask == 0] = 0 # background\n",
    "\n",
    "      border1 = np.ones((int(pr*WIDTH), int(pr*WIDTH)), np.uint8)\n",
    "      border2 = np.ones((int(pr*WIDTH), int(pr*WIDTH)), np.uint8)\n",
    "\n",
    "      # Dilate the foreground to extend the boundary\n",
    "      foreground = cv2.dilate(mask, border1, iterations=1)\n",
    "      # Erode the background to shrink it\n",
    "      backdrop = cv2.erode(mask, border2, iterations=1)\n",
    "\n",
    "      # Adjusting the trimap\n",
    "      bord1 = foreground - mask\n",
    "      bord2 = mask - backdrop\n",
    "      trimap[bord1 == 255] = 128\n",
    "      trimap[bord2 == 255] = 128\n",
    "\n",
    "      # Generating the Alpha matte (for transparency)\n",
    "      gray_area = np.zeros_like(trimap)\n",
    "      gray_area[trimap == 128] = 1 #\n",
    "\n",
    "      mask = mask.astype(np.float32) / 255\n",
    "      mask_3d = cv2.merge([mask,mask,mask])\n",
    "      person = mask_3d * input_image\n",
    "      background = (1 - mask_3d) * target_image\n",
    "      blended = person + background\n",
    "      blended = blended.astype(np.uint8)\n",
    "\n",
    "      final = cv2.inpaint(blended, gray_area, 5, flags=cv2.INPAINT_TELEA)\n",
    "      target_image = final\n",
    "\n",
    "    blended_box = target_image.copy()\n",
    "\n",
    "    # Adding individual boxes for each mask\n",
    "    for box in bounding_boxes:\n",
    "      x, y, w, h = box\n",
    "      cv2.rectangle(blended_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    return target_image, blended_box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZnHVT5ObjTv"
   },
   "source": [
    "# Dataset Generation (Multiple Techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeVcJyxAhe_d"
   },
   "outputs": [],
   "source": [
    "!mkdir poisson\n",
    "!mkdir laplacian\n",
    "!mkdir cutpaste\n",
    "!mkdir inpaint_dib\n",
    "!mkdir boxes\n",
    "!mkdir inpaint_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_boxes(bounding_boxes, dir, name):\n",
    "    lines = []\n",
    "    for i in range(len(bounding_boxes)):\n",
    "      line = \"0 \"\n",
    "      x, y, w, h = bounding_boxes[i]\n",
    "      xc = x + w/2\n",
    "      yc = y + h/2\n",
    "      line += f\"{xc/WIDTH} {yc/HEIGHT} {w/WIDTH} {h/HEIGHT}\\n\"\n",
    "      lines.append(line)\n",
    "    lines = \"\".join(lines)\n",
    "    lines = lines[:-1]\n",
    "    with open(f\"{dir}/{name}.txt\", \"w\") as f:\n",
    "      f.write(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOr-lJrVxGb1"
   },
   "outputs": [],
   "source": [
    "# blending - \"all\", \"cutpaste\", \"poisson\", \"laplacian\"\n",
    "def generate_dataset(input_folder=\"front/\", output_folder=\"back/\", blending=\"all\"):\n",
    "\n",
    "    if blending not in [\"all\", \"inpaint_dib\", \"inpaint_light\", \"poisson\", \"laplacian\",\"cutpaste\"]:\n",
    "        print(\"Sorry we don't have this blending option.\")\n",
    "        exit()\n",
    "\n",
    "    # Getting all the images\n",
    "    input_images = os.listdir(input_folder)\n",
    "    target_images = os.listdir(output_folder)\n",
    "\n",
    "    for i in tqdm(range(len(input_images))):\n",
    "\n",
    "      input_image, input = load_input_image(\"front/\" + input_images[i],HEIGHT, WIDTH)\n",
    "      masks = extract_multiple_mask(input)\n",
    "\n",
    "      for j in range(len(target_images)):\n",
    "        if \".png\" in input_images[i] and \".jpg\" in target_images[j]:\n",
    "          print(f\"Processing {input_images[i]} and {target_images[j]}\")\n",
    "          target_image = cv2.resize(cv2.imread(\"back/\" + target_images[j]),(HEIGHT,WIDTH))\n",
    "          input_image = input_image.astype(np.uint8)\n",
    "          target_image = target_image.astype(np.uint8)\n",
    "          output_name = f\"{input_images[i].split('.')[0]}_{target_images[j].split('.')[0]}\"\n",
    "\n",
    "          # Get the mask from the Inception Model\n",
    "          if masks == []:\n",
    "            continue\n",
    "          final_mask, bounding_box = merge_mask(masks)\n",
    "          write_boxes(bounding_box, \"boxes\", output_name)\n",
    "\n",
    "          if blending == \"cutpaste\" or blending == \"all\":\n",
    "              # Simple cut and paste\n",
    "              simple, simple_boxed = get_simple_cutouts(input_image, target_image, final_mask, bounding_box)\n",
    "              # Simple cut and paste dataset\n",
    "              cv2.imwrite(f\"cutpaste/{output_name}.jpg\",simple)\n",
    "              cv2.imwrite(f\"cutpaste/{output_name}_box.jpg\",simple_boxed)\n",
    "\n",
    "          if blending == \"poisson\" or blending == \"all\":\n",
    "              # Poisson Blending\n",
    "              poisson, poisson_boxed = get_poisson_image(input_image, target_image, final_mask, bounding_box)\n",
    "              # Poisson dataset\n",
    "              cv2.imwrite(f\"poisson/{output_name}.jpg\",poisson)\n",
    "              cv2.imwrite(f\"poisson/{output_name}_box.jpg\",poisson_boxed)\n",
    "\n",
    "          if blending == \"laplacian\" or blending == \"all\":\n",
    "              # Laplacian Blending\n",
    "              laplacian, laplacian_boxed = get_laplacian_image(input_image, target_image, final_mask, bounding_box)\n",
    "              # Laplacian dataset\n",
    "              cv2.imwrite(f\"laplacian/{output_name}.jpg\",laplacian)\n",
    "              cv2.imwrite(f\"laplacian/{output_name}_box.jpg\",laplacian_boxed)\n",
    "\n",
    "          if blending == \"inpaint_dib\" or blending == \"all\":\n",
    "              # Telea Inpainting Blending\n",
    "              telea, telea_boxed = get_inpaint_cutout(input_image, target_image, masks)\n",
    "              # Telea dataset\n",
    "              cv2.imwrite(f\"inpaint_dib/{output_name}.jpg\",telea)\n",
    "              cv2.imwrite(f\"inpaint_dib/{output_name}_box.jpg\",telea_boxed)\n",
    "\n",
    "          if blending == \"inpaint_light\" or blending == \"all\":\n",
    "              # Telea Inpainting Blending\n",
    "              telea, telea_boxed = get_inpaint_cutout(input_image, target_image, masks, light=True)\n",
    "              # Telea dataset\n",
    "              cv2.imwrite(f\"inpaint_light/{output_name}.jpg\",telea)\n",
    "              cv2.imwrite(f\"inpaint_light/{output_name}_box.jpg\",telea_boxed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoDV0DN6DuX2"
   },
   "source": [
    "Generating all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNbYTj5aDyy1"
   },
   "outputs": [],
   "source": [
    "generate_dataset(blending=\"inpaint_dib\") # will generate all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V45CHSjKDv6_"
   },
   "source": [
    "Downloading zip files for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "va-1doC27hQm"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "shutil.make_archive(\"inpaint\".replace('.zip', ''), 'zip', \"inpaint\")\n",
    "zip_filename = \"inpaint.zip\"\n",
    "files.download(zip_filename)\n",
    "\n",
    "shutil.make_archive(\"poisson\".replace('.zip', ''), 'zip', \"poisson\")\n",
    "zip_filename = \"poisson.zip\"\n",
    "files.download(zip_filename)\n",
    "\n",
    "shutil.make_archive(\"laplacian\".replace('.zip', ''), 'zip', \"laplacian\")\n",
    "zip_filename = \"laplacian.zip\"\n",
    "files.download(zip_filename)\n",
    "\n",
    "shutil.make_archive(\"cutpaste\".replace('.zip', ''), 'zip', \"cutpaste\")\n",
    "zip_filename = \"cutpaste.zip\"\n",
    "files.download(zip_filename)\n",
    "\n",
    "shutil.make_archive(\"boxes\".replace('.zip', ''), 'zip', \"boxes\")\n",
    "zip_filename = \"boxes.zip\"\n",
    "files.download(zip_filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
